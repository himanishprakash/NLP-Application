{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3f16137",
   "metadata": {},
   "source": [
    "# Introduction to Next Word Prediction and GPT-2 Fine-tuning\n",
    "\n",
    "Next word prediction is a fundamental task in natural language processing that serves as the foundation for training large language models like GPT-2. This technique works effectively for several reasons:\n",
    "\n",
    "1. **It captures contextual relationships:** By predicting the next word based on previous words, the model learns to understand context and semantic relationships in language.\n",
    "2. **Self-supervised learning:** It allows for unsupervised pre-training on vast amounts of text data without the need for labeled datasets.\n",
    "3. **Generalization:** The skills learned through next word prediction transfer well to various downstream NLP tasks.\n",
    "\n",
    "\n",
    "\n",
    "![Screenshot](/images/1.png)\n",
    "\n",
    "In this project, we're **fine-tuning GPT-2 on a cricket commentary dataset** to generate cricket-specific text. Fine-tuning adapts the pre-trained model to our specific domain, allowing it to generate more relevant and accurate cricket commentary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca047c3",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "We start by loading and cleaning our **cricket commentary data**. \n",
    "\n",
    "Cleaning involves \n",
    "- removing punctuation and special characters to focus on the core textual content. This step is crucial as it helps standardize the input and reduce noise in the data.\n",
    "\n",
    "- splitting data in to train and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "153b2b83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kumarkishalaya/anaconda3/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import re\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9aa8d43",
   "metadata": {},
   "source": [
    "### Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37e29409",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('df_commentary_new.csv')\n",
    "data.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3e1a392",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modified_Commentary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FOUR, first boundary for batsman and team. Ful...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FOUR, back-to-back boundaries to end the first...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FOUR, hit straight back at bowler and he was l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FOUR, another full toss, it's Jadhav this time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FOUR, four more. Jadhav starting to really fin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Modified_Commentary\n",
       "0  FOUR, first boundary for batsman and team. Ful...\n",
       "1  FOUR, back-to-back boundaries to end the first...\n",
       "2  FOUR, hit straight back at bowler and he was l...\n",
       "3  FOUR, another full toss, it's Jadhav this time...\n",
       "4  FOUR, four more. Jadhav starting to really fin..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84172379",
   "metadata": {},
   "source": [
    "Splitting the dataset into training and validation sets\n",
    "\n",
    "For fine-tuning, we need separate datasets for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f9465020",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_text= data.head(1000)\n",
    "df_val_text = data.tail(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "622ffd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove punctuations and special characters\n",
    "def clean_text(text):\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "b0b039de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation text\n",
    "val_text = df_val_text['Modified_Commentary'].apply(clean_text).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c8a330bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FOUR and Saini errs in length He goes full and batsman is just too good there extension of the arms and lofts it over extra cover the ball has enough to trickle into the fence',\n",
       " 'out Caught by de Villiers Oh no batsman has nailed this but straight to the fielder Dubes golden arm strikes again Big call to trust him with this over after Saini had gone for a few but batsman has just thrown it away Charges down converts it into an overpitched delivery and drills it straight to AB at midoff The idea was to place it wide of the fielder like he done to Umesh but batsman was not able to do that batsman c de Villiers b bowler 1718 4s1']"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_text[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "b55dba9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training text\n",
    "train_text = df_train_text['Modified_Commentary'].apply(clean_text).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "c9d1443b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FOUR first boundary for batsman and team Full and on the pads needed to be put away and batsman did just that picked it up and dispatched it over midwicket couple of bounces and into the fence',\n",
       " 'FOUR backtoback boundaries to end the first over Again bowler is a tad short in his length batsman had the width to cut and he didnt try to hit it hard just placed it behind point and Bhuvi at third man gave up the chase pretty quickly']"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fceae7e",
   "metadata": {},
   "source": [
    "## Introduction to GPT-2\n",
    "\n",
    "GPT-2 (Generative Pre-trained Transformer 2) is a large-scale language model developed by OpenAI. It's based on the transformer architecture, which utilizes self-attention mechanisms to process sequential data efficiently. Here are some key details about GPT-2:\n",
    "\n",
    "\n",
    "### Architecture:\n",
    "\n",
    "GPT-2 is a decoder-only transformer model, focusing on generating text based on previous tokens.\n",
    "It uses masked self-attention layers, allowing each token to attend to all previous tokens in the sequence.\n",
    "The model employs layer normalization and residual connections to facilitate training of deep networks.\n",
    "\n",
    "<img src=\"/Users/kumarkishalaya/Desktop/Next_word_prediction/images/Screenshot 2024-07-06 at 6.51.35â€¯AM.png\" alt=\"Screenshot\" width=\"600\"/>\n",
    "\n",
    "### Pre-training:\n",
    "\n",
    "GPT-2 was pre-trained on a diverse dataset called WebText, containing about 8 million web pages.\n",
    "The pre-training objective was next-token prediction, similar to what we're doing in our fine-tuning task.\n",
    "This unsupervised pre-training allows the model to learn general language understanding and generation capabilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d968b217",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "58dac91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset from the text list\n",
    "dataset = Dataset.from_dict({\"text\": train_text})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625df554",
   "metadata": {},
   "source": [
    "#### Preparing Tokenizer to tokenize the data for the model\n",
    "\n",
    "Tokenization is a critical step in preparing our data for the model. It converts the raw text into a format the model can process.\n",
    "\n",
    "Here, we set a **maximum length of 512 tokens** and used padding to ensure all sequences are the same length. This standardization is necessary for batch processing during training. We could experiment with different max_length values to balance between capturing longer contexts and computational efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "12e17df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9abf8c273f6e4bd58156d493218427ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Adding a padding token ensures that all sequences in a batch are of the same length.\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "e5f33a99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2279a25c",
   "metadata": {},
   "source": [
    "#### Create data collator for next word prediction\n",
    "\n",
    "\n",
    "**By setting mlm=False,** we specify that we're doing causal language modeling (next word prediction) rather than masked language modeling. This collator prepares the data in the format required for this specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "8ed62597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep mlm false to train on next word prediction\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dae03f",
   "metadata": {},
   "source": [
    "\n",
    "#### Building Trainer Arguments: Setting up training parameters\n",
    "\n",
    "Trainer arguments define how the model training process should be conducted.\n",
    "\n",
    "This includes settings like **learning rate, batch size, and number of epochs,** which are crucial for effective training.Proper configuration of these parameters can significantly impact the model's performance and convergence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f061d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceadcc7",
   "metadata": {},
   "source": [
    "#### Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ec1a761e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 14:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.742200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.784700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.871600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.906800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.853300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.574100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.736100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.779400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.864200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.672500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.765300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.669300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.767100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.853500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.832900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.690300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.737100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.657500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>1.873100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.933000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>1.784600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>1.862000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>1.984100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>2.112100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.716700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>1.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>1.368900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>1.531300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>1.430700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.461100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>1.442800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>1.482100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>1.388300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>1.452400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.528100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>1.773900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>1.726500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>1.539800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>1.413600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.707400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>1.779100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>1.664500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>1.787600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>1.641800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.744900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>1.922600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>2.332000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>2.393700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>2.437100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.345300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>2.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>1.945000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>1.885100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>1.854400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>1.991100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>1.973900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>2.127900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>1.906500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.887400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>2.035000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>2.147500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>1.959900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>2.151400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.183200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>2.085700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>2.045300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>2.063000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>2.137100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.086600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>2.192100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>2.159900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>2.205500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>2.189300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.176600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1500, training_loss=1.8520173454284667, metrics={'train_runtime': 901.0243, 'train_samples_per_second': 3.33, 'train_steps_per_second': 1.665, 'total_flos': 783876096000000.0, 'train_loss': 1.8520173454284667, 'epoch': 3.0})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02a7ff0",
   "metadata": {},
   "source": [
    "#### Save the model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1a4c4424",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./finetuned_gpt2/tokenizer_config.json',\n",
       " './finetuned_gpt2/special_tokens_map.json',\n",
       " './finetuned_gpt2/vocab.json',\n",
       " './finetuned_gpt2/merges.txt',\n",
       " './finetuned_gpt2/added_tokens.json')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_path = 'code/saved_finetuned_gpt2_model'\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73336393",
   "metadata": {},
   "source": [
    "#### Push model to huggingface hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "cac06f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /Users/kumarkishalaya/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "40ac9354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1238a4065f844fcb9def3b2100db051c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Kumarkishalaya/GPT-2-next-word-prediction/commit/7246f3ecc6e18ad9c2ca85530f61d54810fabccc', commit_message='Upload tokenizer', commit_description='', oid='7246f3ecc6e18ad9c2ca85530f61d54810fabccc', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push the model to the hub\n",
    "model.push_to_hub(\"Kumarkishalaya/GPT-2-next-word-prediction\")\n",
    "tokenizer.push_to_hub(\"Kumarkishalaya/GPT-2-next-word-prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7212d0e",
   "metadata": {},
   "source": [
    "#### Loading models again for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "c3aad4a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_load_path = 'code/saved_finetuned_gpt2_model'\n",
    "model_finetuned = GPT2LMHeadModel.from_pretrained(model_load_path)\n",
    "tokenizer_finetuned = GPT2Tokenizer.from_pretrained(model_load_path)\n",
    "\n",
    "model_name = 'gpt2'\n",
    "model_before_finetuning = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer_before_finetuning = GPT2Tokenizer.from_pretrained(model_load_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdea2dcd",
   "metadata": {},
   "source": [
    "### Evaluation using perplexity and comparing a finetuned and a base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "b33210c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate perplexity\n",
    "def calculate_perplexity(text, model, tokenizer):\n",
    "    encodings = tokenizer(text, return_tensors='pt')\n",
    "    max_length = model.config.n_positions\n",
    "    stride = 512\n",
    "    seq_len = encodings.input_ids.size(1)\n",
    "    \n",
    "    nlls = []\n",
    "    prev_end_loc = 0\n",
    "    for begin_loc in range(0, seq_len, stride):\n",
    "        end_loc = min(begin_loc + max_length, seq_len)\n",
    "        trg_len = end_loc - prev_end_loc  # may be different from stride on last iteration\n",
    "        input_ids = encodings.input_ids[:, begin_loc:end_loc]\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[:, :-trg_len] = -100\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=target_ids)\n",
    "            \n",
    "            # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
    "            # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
    "            # to the left by 1.\n",
    "            neg_log_likelihood = outputs.loss * trg_len\n",
    "        \n",
    "        nlls.append(neg_log_likelihood)\n",
    "        \n",
    "        prev_end_loc = end_loc\n",
    "        if end_loc == seq_len:\n",
    "            break\n",
    "    \n",
    "    ppl = torch.exp(torch.stack(nlls).sum() / end_loc)\n",
    "    return ppl.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "4027c654",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_finetuned.to(device)\n",
    "model_before_finetuning.to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model_finetuned.eval()\n",
    "model_before_finetuning.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "261f1a14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FOUR and Saini errs in length He goes full and batsman is just too good there extension of the arms and lofts it over extra cover the ball has enough to trickle into the fence',\n",
       " 'out Caught by de Villiers Oh no batsman has nailed this but straight to the fielder Dubes golden arm strikes again Big call to trust him with this over after Saini had gone for a few but batsman has just thrown it away Charges down converts it into an overpitched delivery and drills it straight to AB at midoff The idea was to place it wide of the fielder like he done to Umesh but batsman was not able to do that batsman c de Villiers b bowler 1718 4s1']"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_text[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "d733b903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate perplexity for each text\n",
    "perplexity_finetuned_train_data = [calculate_perplexity(text,model_finetuned,tokenizer_finetuned) for text in train_text]\n",
    "perplexity_finetuned = [calculate_perplexity(text,model_finetuned,tokenizer_finetuned) for text in val_text]\n",
    "perplexity_base = [calculate_perplexity(text,model_before_finetuning,tokenizer_before_finetuning) for text in val_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "91f1ee71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average perplexity on training_data: 5.20\n",
      "Average perplexity on evalulation data: 223.62\n",
      "Average perplexity on evaluation data (basemodel): 574.18\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print average perplexity\n",
    "avg_perplexity_train_data = np.mean(perplexity_finetuned_train_data)\n",
    "print(f\"Average perplexity on training_data: {avg_perplexity_train_data:.2f}\")\n",
    "\n",
    "\n",
    "avg_perplexity_finetuned = np.mean(perplexity_finetuned)\n",
    "print(f\"Average perplexity on evalulation data: {avg_perplexity_finetuned:.2f}\")\n",
    "\n",
    "avg_perplexity_base = np.mean(perplexity_base)\n",
    "print(f\"Average perplexity on evaluation data (basemodel): {avg_perplexity_base:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5272c2d3",
   "metadata": {},
   "source": [
    "\n",
    "Perplexity measures how well the model predicts a sample. **Lower perplexity indicates better performance**. We calculate perplexity for three scenarios:\n",
    "\n",
    "1. **Fine-tuned model on training data**: This shows how well the model has learned the training set. **The lowest perplexity here (5.20)** indicates the model has fit well to the training data.\n",
    "\n",
    "2. **Fine-tuned model on evaluation data:** This measures how well the model generalizes to unseen data in the same domain. **The higher perplexity (223.62)** compared to the training data suggests some overfitting, but it's still an improvement over the base model.\n",
    "\n",
    "3. **Base model on evaluation data:** This serves as a baseline, showing the performance before fine-tuning. **The highest perplexity here (574.18)** demonstrates that fine-tuning has indeed improved the model's performance on cricket commentary.\n",
    "\n",
    "The significant improvement from the base model to the fine-tuned model on evaluation data shows the value of domain-specific fine-tuning. However, the gap between training and evaluation perplexity for the fine-tuned model suggests there's room for improvement in generalization, possibly through techniques like regularization or using a larger and more diverse training dataset.\n",
    "\n",
    "This fine-tuning process demonstrates how we can adapt a powerful language model like GPT-2 to a specific domain. While we've focused on next word prediction, this fine-tuned model could potentially be used for various downstream tasks related to cricket commentary, such as generating full commentaries, summarizing match highlights, or even answering cricket-related questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ac67e6",
   "metadata": {},
   "source": [
    "#### Playing around with output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8e821e6b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_finetuned.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "9bac540c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'how about that for a shot Just a hint of width outside off and batsman takes it on the back foot and sends it sailing over deep midwicket No need to run for shots like that Length delivery that sits up to be smacked batsman'"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = 'how about that for a shot'\n",
    "input_ids = tokenizer_finetuned(input_text, return_tensors=\"pt\")\n",
    "input_ids = input_ids['input_ids'].to(device)\n",
    "output = model_finetuned.generate(input_ids, max_length=50, num_beams=5, do_sample=True)\n",
    "loaded_tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a728c2e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_before_finetuning = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model_before_finetuning.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "e468acee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'how about that for a shot?\"\\n\\n\"I don\\'t think so.\"\\n\\n\"I don\\'t think so at all.\"\\n\\n\"You don\\'t think so?\"\\n\\n\"I'"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = 'how about that for a shot'\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "input_ids = input_ids['input_ids'].to(device)\n",
    "output = model_before_finetuning.generate(input_ids, max_length=40, num_beams=5, do_sample=True)\n",
    "tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c29d562",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
